"""
ë¬¸ì„œ ë¡œë” ì›¹ UI í…ŒìŠ¤íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë¶„ì„í•˜ë©° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼
Streamlitì„ í†µí•œ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import tempfile
import time
import streamlit as st
from pathlib import Path
from typing import Dict, Any, List
from dotenv import load_dotenv, find_dotenv, set_key, get_key

from module.document_parser import load_documents
from module.vector_db import VectorDB
from langchain.schema import Document

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(find_dotenv(), override=True)


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë¬¸ì„œ ë¡œë” í…ŒìŠ¤íŠ¸",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”ã…
if 'documents' not in st.session_state:
    st.session_state.documents = None
if 'stats' not in st.session_state:
    st.session_state.stats = {}
if 'vector_db' not in st.session_state:
    st.session_state.vector_db = None
# DB ê²½ë¡œ ê³ ì •
st.session_state.db_path = "./db/faiss"


def init_vector_db(force_new=False):
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    
    Args:
        force_new: ê°•ì œë¡œ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì—¬ë¶€
    """
    if force_new or st.session_state.vector_db is None:
        # ê¸°ì¡´ ê°ì²´ ì œê±°
        if 'vector_db' in st.session_state:
            del st.session_state['vector_db']
            
        # ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (force_new ì˜µì…˜ ì „ë‹¬)
        st.session_state.vector_db = VectorDB(
            storage_path=st.session_state.db_path,
            force_new=force_new  # ì´ ê°’ì´ Trueë©´ ë””ìŠ¤í¬ì— íŒŒì¼ì´ ìˆì–´ë„ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ë§Œë“¬
        )
        
    return st.session_state.vector_db


def display_document_stats(docs: List[Document], stats: Dict[str, Any]):
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ì›ë³¸ ë¬¸ì„œ", stats.get('original_document_count', len(docs)))
    with col2:
        st.metric("ì²­í¬ ë¬¸ì„œ", stats.get('chunked_document_count', len(docs)))
    with col3:
        st.metric("ì´ ë¬¸ì ìˆ˜", f"{stats.get('total_characters', 0):,}")
    with col4:
        st.metric("í‰ê·  ë¬¸ì„œ ê¸¸ì´", f"{stats.get('average_doc_length', 0):,} ë¬¸ì")
    
    # íŒŒì¼ ìœ í˜• í†µê³„
    st.subheader("ğŸ“‘ íŒŒì¼ ìœ í˜•")
    detected_types = stats.get('detected_types', {})
    if detected_types:
        st.json(detected_types)
    else:
        st.info("íŒŒì¼ ìœ í˜• ì •ë³´ ì—†ìŒ")
    
    # ì´ë¯¸ì§€ ë° í‘œ í†µê³„
    image_count = 0
    image_saved_count = 0
    table_count = 0
    
    for doc in docs:
        images = doc.metadata.get("images", [])
        tables = doc.metadata.get("tables", [])
        image_count += len(images)
        
        # ì‹¤ì œë¡œ ì €ì¥ëœ ì´ë¯¸ì§€ë§Œ ì¹´ìš´íŠ¸
        for img in images:
            if "image_path" in img and Path(img["image_path"]).exists():
                image_saved_count += 1
                
        table_count += len(tables)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì¶”ì¶œëœ ì´ë¯¸ì§€ ì´ìˆ˜", image_count)
    with col2:
        st.metric("ì €ì¥ëœ ì´ë¯¸ì§€ íŒŒì¼", image_saved_count)
    with col3:
        st.metric("ì¶”ì¶œëœ í‘œ", table_count)


def display_document_preview(docs: List[Document], limit: int = 3):
    """ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ"""
    st.subheader("ğŸ‘ï¸ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°")
    
    # ë¯¸ë¦¬ë³¼ ë¬¸ì„œ ìˆ˜ ì„ íƒ
    preview_limit = min(limit, len(docs))
    
    for i, doc in enumerate(docs[:preview_limit]):
        with st.expander(f"ë¬¸ì„œ {i+1}/{preview_limit}", expanded=i==0):
            # ë‚´ìš©
            st.markdown("**ğŸ“„ ë‚´ìš©:**")
            st.text_area(
                label=f"ë¬¸ì„œ ë‚´ìš©",
                value=doc.page_content[:3000] + ("..." if len(doc.page_content) > 3000 else ""),
                height=200,
                key=f"doc_content_{i}"
            )
            
            # ë©”íƒ€ë°ì´í„°
            st.markdown("**â„¹ï¸ ë©”íƒ€ë°ì´í„°:**")
            meta_preview = {k: v for k, v in doc.metadata.items() 
                           if k not in ["images", "tables"]}
            st.json(meta_preview)
            
            # ì´ë¯¸ì§€ ì •ë³´
            images = doc.metadata.get("images", [])
            if images:
                st.markdown(f"**ğŸ–¼ï¸ ì´ë¯¸ì§€ ({len(images)}ê°œ):**")
                for j, img in enumerate(images[:3]):
                    cols = st.columns([1, 2])
                    with cols[0]:
                        st.markdown(f"**ì´ë¯¸ì§€ {j+1}:**")
                        st.write(f"í¬ê¸°: {img.get('width')}x{img.get('height')}")
                        st.write(f"í˜•ì‹: {img.get('image_format', 'unknown')}")
                    
                    with cols[1]:
                        image_path = img.get("image_path")
                        if image_path and Path(image_path).exists():
                            st.image(image_path, width=300)
                        else:
                            st.info("ì´ë¯¸ì§€ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        
                        ocr_text = img.get("ocr_text")
                        if ocr_text:
                            with st.expander("OCR í…ìŠ¤íŠ¸"):
                                st.text(ocr_text)
                
                if len(images) > 3:
                    st.info(f"... ì™¸ {len(images) - 3}ê°œ ì´ë¯¸ì§€ (ëª¨ë‘ í‘œì‹œí•˜ì§€ ì•ŠìŒ)")
            
            # í‘œ ì •ë³´
            tables = doc.metadata.get("tables", [])
            if tables:
                st.markdown(f"**ğŸ“Š í‘œ ({len(tables)}ê°œ):**")
                for j, table in enumerate(tables[:3]):
                    with st.expander(f"í‘œ {j+1}: {table.get('rows')}í–‰ x {table.get('columns')}ì—´"):
                        # í‘œ ë°ì´í„°ë¥¼ Streamlit í…Œì´ë¸”ë¡œ í‘œì‹œ
                        if "data" in table:
                            st.table(table["data"])
                        else:
                            st.text(table.get("text", "í‘œ ë°ì´í„° ì—†ìŒ"))
                
                if len(tables) > 3:
                    st.info(f"... ì™¸ {len(tables) - 3}ê°œ í‘œ (ëª¨ë‘ í‘œì‹œí•˜ì§€ ì•ŠìŒ)")


def main():
    st.title("ğŸ“š ë¬¸ì„œ ë¡œë” í…ŒìŠ¤íŠ¸")

    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # OpenAI API Key
        openai_key = st.text_input(
            "OpenAI API Key", 
            type="password",
            value=get_key(find_dotenv(), 'OPENAI_API_KEY') or '',
            help="ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ OpenAI API í‚¤"
        )
        if openai_key:
            set_key(find_dotenv(), 'OPENAI_API_KEY', openai_key)
        
        # ì²­í¬ ì„¤ì •
        st.subheader("ğŸ“ ì²­í¬ ì„¤ì •")
        chunk_size = st.slider("ì²­í¬ í¬ê¸°", 500, 2000, 1000, 100)
        chunk_overlap = st.slider("ì²­í¬ ì˜¤ë²„ë©", 50, 500, 200, 50)
        
        # PDF ì²˜ë¦¬ ê¸°ë³¸ ì„¤ì • (ì‚¬ìš©ì UI ì—†ìŒ)
        extract_images = True
        extract_tables = True
        split_documents = True
        
        image_dir = None  # ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥ ì•ˆí•¨
        
        st.divider()
        
        # ë²¡í„° DB ìƒíƒœ
        st.subheader("ğŸ—‚ï¸ ë²¡í„° DB ìƒíƒœ")
        if st.button("ğŸ”„ ìƒíƒœ ìƒˆë¡œê³ ì¹¨"):
            vector_db = init_vector_db()
            stats = vector_db.get_stats()
            st.json(stats)
    
    # OpenAI API í‚¤ í™•ì¸
    if not get_key(find_dotenv(), 'OPENAI_API_KEY'):
        st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •í•´ì£¼ì„¸ìš”!")
    
    # ë©”ì¸ íƒ­
    tab1, tab2, tab3 = st.tabs(["ğŸ“„ ë¬¸ì„œ ë¡œë”©", "ğŸ” ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°", "ğŸ—‚ï¸ ë²¡í„° DB ì €ì¥"])
    
    with tab1:
        st.header("ğŸ“„ ë¬¸ì„œ ë¡œë”©")
        
        # íŒŒì¼ ì—…ë¡œë“œ ì˜µì…˜
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
            uploaded_files = st.file_uploader(
                "ë¬¸ì„œ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
                type=['pdf', 'txt', 'docx', 'pptx', 'xlsx', 'csv', 'md', 'html', 'json'],
                help="Magika AIê°€ íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ íƒ€ì…ì„ ê²°ì •í•©ë‹ˆë‹¤",
                accept_multiple_files=True
            )
        
        with col2:
            st.subheader("ğŸ“‚ ìƒ˜í”Œ íŒŒì¼")
            sample_path = "sample/êµ­ê°€ë³„ ê³µê³µë¶€ë¬¸ AI ë„ì… ë° í™œìš© ì „ëµ.pdf"
            if Path(sample_path).exists():
                if st.button("ğŸ‡°ğŸ‡· ìƒ˜í”Œ PDF ì‚¬ìš©", type="primary"):
                    st.session_state.selected_file = sample_path
                    st.success(f"ìƒ˜í”Œ íŒŒì¼ ì„ íƒë¨: {sample_path}")
            else:
                st.error(f"ìƒ˜í”Œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sample_path}")
        
        # ì²˜ë¦¬í•  íŒŒì¼ ê²°ì •
        files_to_process = []
        
        if uploaded_files:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ ì €ì¥
            for uploaded_file in uploaded_files:
                with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{uploaded_file.name}") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    files_to_process.append({
                        'path': tmp_file.name,
                        'name': uploaded_file.name,
                        'temp': True
                    })
            
            st.info(f"ğŸ“¤ ì—…ë¡œë“œë¨: {len(files_to_process)}ê°œ íŒŒì¼")
            for i, file_info in enumerate(files_to_process):
                st.text(f"  {i+1}. {file_info['name']}")
        
        elif hasattr(st.session_state, 'selected_file'):
            files_to_process.append({
                'path': st.session_state.selected_file,
                'name': Path(st.session_state.selected_file).name,
                'temp': False
            })
        
        if files_to_process and st.button("ğŸš€ ë¬¸ì„œ ë¡œë”© ì‹¤í–‰", type="primary"):
            all_documents = []
            all_stats = {
                'original_document_count': 0,
                'chunked_document_count': 0,
                'total_characters': 0,
                'detected_types': {},
                'total_sources': 0
            }
            
            # ê° ë¬¸ì„œë³„ í†µê³„ë¥¼ ì €ì¥í•  ë³€ìˆ˜
            individual_results = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                for i, file_info in enumerate(files_to_process):
                    status_text.text(f"ğŸ“Š {i+1}/{len(files_to_process)} - {file_info['name']} ì²˜ë¦¬ ì¤‘...")
                    
                    with st.spinner(f"ğŸ“„ {file_info['name']} ë¶„ì„ ì¤‘..."):
                        documents, stats = load_documents(
                            path=file_info['path'],
                            chunk_size=chunk_size,
                            chunk_overlap=chunk_overlap,
                            split_documents=split_documents,
                            extract_images=extract_images,
                            extract_tables=extract_tables,
                            image_dir=image_dir if extract_images else None
                        )
                        
                        # ê°œë³„ ë¬¸ì„œ í†µê³„ ì €ì¥ (í‰ê·  ë¬¸ì„œ ê¸¸ì´ë¥¼ ì²­í¬ ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°)
                        if stats.get('chunked_document_count', 0) > 0:
                            stats['average_doc_length'] = stats.get('total_characters', 0) // stats.get('chunked_document_count', 1)
                        else:
                            stats['average_doc_length'] = 0
                            
                        individual_results.append({
                            'file_name': file_info['name'],
                            'documents': documents,
                            'stats': stats
                        })
                        
                        # í†µê³„ ë³‘í•©
                        all_documents.extend(documents)
                        all_stats['original_document_count'] += stats.get('original_document_count', 0)
                        all_stats['chunked_document_count'] += stats.get('chunked_document_count', 0)
                        all_stats['total_characters'] += stats.get('total_characters', 0)
                        all_stats['total_sources'] += 1
                        
                        # íŒŒì¼ ìœ í˜• ë³‘í•©
                        for dtype, count in stats.get('detected_types', {}).items():
                            if dtype in all_stats['detected_types']:
                                all_stats['detected_types'][dtype] += count
                            else:
                                all_stats['detected_types'][dtype] = count
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    progress_bar.progress((i + 1) / len(files_to_process))
                
                # í‰ê·  ë¬¸ì„œ ê¸¸ì´ ê³„ì‚°
                if all_stats['chunked_document_count'] > 0:
                    all_stats['average_doc_length'] = all_stats['total_characters'] // all_stats['chunked_document_count']
                else:
                    all_stats['average_doc_length'] = 0
                    
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.documents = all_documents
                st.session_state.stats = all_stats
                st.session_state.individual_results = individual_results
                
                # ê²°ê³¼ í‘œì‹œ
                status_text.text("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
                progress_bar.progress(100)
                st.success(f"âœ… ì„±ê³µ! {len(all_documents)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
                
                # ì „ì²´ í†µê³„
                st.subheader("ğŸ“Š ì „ì²´ í†µê³„")
                display_document_stats(all_documents, all_stats)
                
                # ê° ë¬¸ì„œë³„ í†µê³„
                st.subheader("ğŸ“‚ ê° ë¬¸ì„œë³„ í†µê³„")
                for idx, result in enumerate(individual_results):
                    with st.expander(f"{idx+1}. {result['file_name']}", expanded=(idx==0)):
                        display_document_stats(result['documents'], result['stats'])
                
            except Exception as e:
                st.error(f"âŒ ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                for file_info in files_to_process:
                    if file_info.get('temp', False) and Path(file_info['path']).exists():
                        try:
                            Path(file_info['path']).unlink(missing_ok=True)
                        except:
                            pass
    
    with tab2:
        st.header("ğŸ” ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°")
        
        if st.session_state.documents:
            documents = st.session_state.documents
            
            # ë¯¸ë¦¬ë³¼ ë¬¸ì„œ ìˆ˜ ì„ íƒ
            preview_count = st.slider("ë¯¸ë¦¬ë³¼ ë¬¸ì„œ ìˆ˜", 1, min(10, len(documents)), 3)
            
            # ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
            display_document_preview(documents, limit=preview_count)
            
        else:
            st.info("ë¨¼ì € 'ë¬¸ì„œ ë¡œë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    with tab3:
        st.header("ğŸ—‚ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
        
        # ë²¡í„° DB ì´ˆê¸°í™”
        vector_db = init_vector_db()
        
        # í˜„ì¬ DB ìƒíƒœ í‘œì‹œ
        col1, col2 = st.columns([4, 1])
        with col1:
            st.subheader("ğŸ“Š í˜„ì¬ ìƒíƒœ")
        with col2:
            if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", key="refresh_status"):
                st.rerun()
                
        stats = vector_db.get_stats()
        st.json(stats)
        
        is_empty = vector_db.is_empty()
        if is_empty:
            st.info("ğŸ“­ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        else:
            st.success("ğŸ“š ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤")
        
        # ë¬¸ì„œ ì¶”ê°€
        st.subheader("ğŸ”§ ì‘ì—…")
        
        if st.session_state.documents:
            documents = st.session_state.documents
            
            if st.button("ğŸ“ ë¬¸ì„œ ì¶”ê°€", type="primary"):
                try:
                    with st.spinner("ë²¡í„° ì„ë² ë”© ìƒì„± ì¤‘..."):
                        vector_db.add_documents(documents)
                    
                    st.success(f"âœ… {len(documents)}ê°œ ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ì €ì¥
                    with st.spinner("ë””ìŠ¤í¬ì— ì €ì¥ ì¤‘..."):
                        vector_db.save()
                    st.success("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ë””ìŠ¤í¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # 2ì´ˆ í›„ ìë™ ìƒˆë¡œê³ ì¹¨
                    with st.spinner("í™”ë©´ ê°±ì‹  ì¤‘..."):
                        time.sleep(1)
                        st.rerun()
                    
                    # ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
                    st.subheader("ğŸ“Š ì—…ë°ì´íŠ¸ëœ ìƒíƒœ")
                    new_stats = vector_db.get_stats()
                    st.json(new_stats)
                    
                except Exception as e:
                    st.error(f"âŒ ë²¡í„° DB ì‘ì—… ì‹¤íŒ¨: {str(e)}")
        else:
            st.warning("âš ï¸ ë¨¼ì € 'ë¬¸ì„œ ë¡œë”©' íƒ­ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        st.markdown("---")
        
        if st.button("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”", key="reset_db_button", type="secondary"):
            try:
                # force_new=Trueë¡œ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                init_vector_db(force_new=True)
                
                # ì„±ê³µ ë©”ì‹œì§€ í‘œì‹œ 
                st.success("ğŸ”„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ìƒˆë¡œê³ ì¹¨
                time.sleep(1)
                st.rerun()
                
            except Exception as e:
                st.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
                import traceback
                st.code(traceback.format_exc())
                st.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
                import traceback
                st.code(traceback.format_exc())


if __name__ == "__main__":
    main()
