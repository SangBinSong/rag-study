"""
LangChainì„ ì‚¬ìš©í•œ ë²”ìš© ë¬¸ì„œ ë¡œë” ëª¨ë“ˆ
PDF, TXT, DOCX, HTML, CSV ë“± ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import logging
from pathlib import Path
from typing import List, Union, Dict, Any
from langchain_community.document_loaders import (
    TextLoader,
    CSVLoader,
    UnstructuredHTMLLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredPowerPointLoader,
    UnstructuredExcelLoader,
    UnstructuredMarkdownLoader,
    JSONLoader
)
from langchain.document_loaders import PyMuPDFLoader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from magika import Magika

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentLoader:
    """AI ê¸°ë°˜ íŒŒì¼ íƒ€ì… ê²€ì¶œì„ ì§€ì›í•˜ëŠ” ë‹¤ì¤‘ í¬ë§· ë¬¸ì„œ ë¡œë”"""
    
    # Magika íŒŒì¼ íƒ€ì…ê³¼ LangChain ë¡œë” ë§¤í•‘
    MAGIKA_LOADER_MAPPING = {
        'pdf': PyMuPDFLoader,
        'txt': TextLoader,
        'csv': CSVLoader,
        'html': UnstructuredHTMLLoader,
        'docx': UnstructuredWordDocumentLoader,
        'doc': UnstructuredWordDocumentLoader,
        'pptx': UnstructuredPowerPointLoader,
        'ppt': UnstructuredPowerPointLoader,
        'xlsx': UnstructuredExcelLoader,
        'xls': UnstructuredExcelLoader,
        'markdown': UnstructuredMarkdownLoader,
        'json': JSONLoader
    }
    
    # í™•ì¥ì ê¸°ë°˜ í´ë°± ë§¤í•‘ (Magika ê²€ì¶œ ì‹¤íŒ¨ ì‹œ)
    EXTENSION_LOADER_MAPPING = {
        '.pdf': PyMuPDFLoader,
        '.txt': TextLoader,
        '.csv': CSVLoader,
        '.html': UnstructuredHTMLLoader,
        '.htm': UnstructuredHTMLLoader,
        '.docx': UnstructuredWordDocumentLoader,
        '.doc': UnstructuredWordDocumentLoader,
        '.pptx': UnstructuredPowerPointLoader,
        '.ppt': UnstructuredPowerPointLoader,
        '.xlsx': UnstructuredExcelLoader,
        '.xls': UnstructuredExcelLoader,
        '.md': UnstructuredMarkdownLoader,
        '.json': JSONLoader
    }
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™”
        
        Args:
            chunk_size: í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  í¬ê¸°
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        # AI ê¸°ë°˜ íŒŒì¼ íƒ€ì… ê²€ì¶œì„ ìœ„í•œ Magika ì´ˆê¸°í™”
        self.magika = Magika()
    
    def _detect_file_type(self, file_path: Path) -> str:
        """
        Magika AIë¥¼ ì‚¬ìš©í•œ íŒŒì¼ íƒ€ì… ê²€ì¶œ
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê²€ì¶œëœ íŒŒì¼ íƒ€ì… ë¬¸ìì—´
        """
        try:
            result = self.magika.identify_path(file_path)
            detected_type = result.output.ct_label
            logger.info(f"Magika detected file type: {detected_type} for {file_path}")
            return detected_type
        except Exception as e:
            logger.warning(f"Magika detection failed for {file_path}: {str(e)}")
            return None
    
    def load_document(
        self, 
        file_path: Union[str, Path],
        encoding: str = 'utf-8',
        **kwargs
    ) -> List[Document]:
        """
        íŒŒì¼ ê²½ë¡œì—ì„œ ë‹¨ì¼ ë¬¸ì„œ ë¡œë“œ
        
        Args:
            file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            encoding: í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ê¸°ë³¸ê°’: utf-8)
            **kwargs: íŠ¹ì • ë¡œë”ìš© ì¶”ê°€ ì¸ì
            
        Returns:
            Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì¸ ê²½ìš°
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Magika AI ê¸°ë°˜ ê²€ì¶œë§Œ ì‚¬ìš©
        detected_type = self._detect_file_type(file_path)
        
        if not detected_type or detected_type not in self.MAGIKA_LOADER_MAPPING:
            raise ValueError(f"Magikaê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì„ ê²€ì¶œí–ˆìŠµë‹ˆë‹¤: {detected_type}")
        
        loader_class = self.MAGIKA_LOADER_MAPPING[detected_type]
        logger.info(f"Magika ê²€ì¶œ ì‚¬ìš©: {detected_type}")
        
        try:
            # ê²€ì¶œëœ íƒ€ì…ì— ë”°ë¥¸ íŠ¹ì • ë¡œë” ìš”êµ¬ì‚¬í•­ ì²˜ë¦¬
            if detected_type == 'txt':
                loader = loader_class(str(file_path), encoding=encoding)
            elif detected_type == 'csv':
                loader = loader_class(str(file_path), encoding=encoding, **kwargs)
            elif detected_type == 'json':
                # JSON ë¡œë”ëŠ” jq_schema ë§¤ê°œë³€ìˆ˜ê°€ í•„ìš”
                jq_schema = kwargs.get('jq_schema', '.')
                loader = loader_class(str(file_path), jq_schema=jq_schema)
            else:
                loader = loader_class(str(file_path), **kwargs)
            
            documents = loader.load()
            
            # Magika ê²€ì¶œ ê²°ê³¼ë¥¼ í¬í•¨í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for doc in documents:
                doc.metadata.update({
                    'source': str(file_path),
                    'detected_type': detected_type,
                    'file_size': file_path.stat().st_size,
                    'file_name': file_path.name,
                    'detection_method': 'magika'
                })
            
            logger.info(f"Successfully loaded {len(documents)} documents from {file_path}")
            return documents
            
        except Exception as e:
            logger.error(f"Error loading document {file_path}: {str(e)}")
            raise
    
    def load_directory(
        self,
        directory_path: Union[str, Path],
        glob_pattern: str = "*",
        recursive: bool = True,
        show_progress: bool = False,
        **kwargs
    ) -> List[Document]:
        """
        Load all supported documents from a directory
        
        Args:
            directory_path: Path to directory
            glob_pattern: Pattern to match files (default: "*")
            recursive: Whether to search subdirectories
            show_progress: Whether to show loading progress
            **kwargs: Additional arguments for loaders
            
        Returns:
            List of Document objects from all loaded files
        """
        directory_path = Path(directory_path)
        
        if not directory_path.exists() or not directory_path.is_dir():
            raise NotADirectoryError(f"Directory not found: {directory_path}")
        
        all_documents = []
        supported_magika_types = list(self.MAGIKA_LOADER_MAPPING.keys())
        
        # Find all files (let Magika decide if they're supported)
        pattern = "**/*" if recursive else "*"
        files = [f for f in directory_path.glob(pattern) if f.is_file()]
        
        if not files:
            logger.warning(f"No supported files found in {directory_path}")
            return []
        
        logger.info(f"Found {len(files)} supported files in {directory_path}")
        
        processed_files = 0
        skipped_files = 0
        
        for file_path in files:
            if show_progress:
                print(f"Processing: {file_path.name}")
            
            try:
                # Check if file is supported by Magika detection
                detected_type = self._detect_file_type(file_path)
                
                if detected_type and detected_type in supported_magika_types:
                    documents = self.load_document(file_path, **kwargs)
                    all_documents.extend(documents)
                    processed_files += 1
                    if show_progress:
                        print(f"âœ… Loaded: {file_path.name} ({detected_type})")
                else:
                    skipped_files += 1
                    if show_progress:
                        print(f"â­ï¸ Skipped: {file_path.name} (unsupported: {detected_type})")
                        
            except Exception as e:
                logger.error(f"Failed to load {file_path}: {str(e)}")
                skipped_files += 1
                continue
        
        logger.info(f"Processing complete: {processed_files} loaded, {skipped_files} skipped, {len(all_documents)} total documents")
        return all_documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        Split documents into smaller chunks
        
        Args:
            documents: List of Document objects to split
            
        Returns:
            List of split Document objects
        """
        if not documents:
            return []
        
        split_docs = self.text_splitter.split_documents(documents)
        logger.info(f"Split {len(documents)} documents into {len(split_docs)} chunks")
        return split_docs
    
    def load_and_split(
        self,
        file_path: Union[str, Path],
        **kwargs
    ) -> List[Document]:
        """
        Convenience method to load and split a document in one step
        
        Args:
            file_path: Path to the document file
            **kwargs: Additional arguments for loader
            
        Returns:
            List of split Document objects
        """
        documents = self.load_document(file_path, **kwargs)
        return self.split_documents(documents)
    
    def get_supported_formats(self) -> Dict[str, List[str]]:
        """
        Get list of supported file formats
        
        Returns:
            Dictionary with magika types and file extensions
        """
        return {
            'magika_types': list(self.MAGIKA_LOADER_MAPPING.keys()),
            'extensions': list(self.EXTENSION_LOADER_MAPPING.keys())
        }
    
    def get_document_info(self, documents: List[Document]) -> Dict[str, Any]:
        """
        Get information about loaded documents
        
        Args:
            documents: List of Document objects
            
        Returns:
            Dictionary with document statistics
        """
        if not documents:
            return {}
        
        total_chars = sum(len(doc.page_content) for doc in documents)
        file_types = {}
        detected_types = {}
        detection_methods = {'magika': 0, 'extension': 0}
        sources = set()
        
        for doc in documents:
            # Magika detection stats
            detected_type = doc.metadata.get('detected_type', 'unknown')
            detected_types[detected_type] = detected_types.get(detected_type, 0) + 1
            
            # Detection method stats  
            method = doc.metadata.get('detection_method', 'unknown')
            if method in detection_methods:
                detection_methods[method] += 1
                
            sources.add(doc.metadata.get('source', 'unknown'))
        
        return {
            'total_documents': len(documents),
            'total_characters': total_chars,
            'total_sources': len(sources),
            'detected_types': detected_types,
            'detection_methods': detection_methods,
            'average_doc_length': total_chars // len(documents) if documents else 0
        }


def load_documents(
    path: Union[str, Path],
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    split_documents: bool = True,
    **kwargs
) -> List[Document]:
    """
    íŒŒì¼ ë˜ëŠ” ë””ë ‰í„°ë¦¬ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        path: íŒŒì¼ ë˜ëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ
        chunk_size: í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  í¬ê¸°
        chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
        split_documents: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í• ì§€ ì—¬ë¶€
        **kwargs: ë¡œë”ìš© ì¶”ê°€ ì¸ì
        
    Returns:
        Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    loader = DocumentLoader(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    path = Path(path)
    
    if path.is_file():
        documents = loader.load_document(path, **kwargs)
    elif path.is_dir():
        documents = loader.load_directory(path, **kwargs)
    else:
        raise ValueError(f"Path does not exist: {path}")
    
    if split_documents:
        documents = loader.split_documents(documents)
    
    return documents


if __name__ == "__main__":
    # ì‹¤ì œ PDF íŒŒì¼ì„ ì‚¬ìš©í•œ ì˜ˆì œ
    loader = DocumentLoader(chunk_size=1000, chunk_overlap=200)
    
    # ì§€ì› í˜•ì‹ í™•ì¸
    print("ğŸ”§ ì§€ì› í˜•ì‹:", loader.get_supported_formats())
    print()
    
    # ì˜ˆì œ 1: Magika ê²€ì¶œì„ ì‚¬ìš©í•œ ë‹¨ì¼ PDF íŒŒì¼ ë¡œë“œ
    try:
        pdf_path = "sample/êµ­ê°€ë³„ ê³µê³µë¶€ë¬¸ AI ë„ì… ë° í™œìš© ì „ëµ.pdf"
        print(f"ğŸ“„ PDF ë¡œë”©: {pdf_path}")
        documents = loader.load_and_split(pdf_path)
        
        print(f"âœ… ì„±ê³µì ìœ¼ë¡œ {len(documents)}ê°œ ë¬¸ì„œ ì²­í¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤")
        
        # ì²« ë²ˆì§¸ ë¬¸ì„œ ì •ë³´ í‘œì‹œ
        if documents:
            first_doc = documents[0]
            print(f"ğŸ“Š ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì):")
            print(f"   ë‚´ìš©: {first_doc.page_content[:200]}...")
            print(f"   ë©”íƒ€ë°ì´í„°: {first_doc.metadata}")
        print()
        
        # ë¬¸ì„œ í†µê³„ í™•ì¸
        info = loader.get_document_info(documents)
        print("ğŸ“ˆ ë¬¸ì„œ í†µê³„:")
        for key, value in info.items():
            print(f"   {key}: {value}")
        print()
        
    except Exception as e:
        print(f"âŒ PDF ë¡œë”© ì˜¤ë¥˜: {e}")
        print()
    
    # ì˜ˆì œ 2: ìƒ˜í”Œ ë””ë ‰í„°ë¦¬ì˜ ëª¨ë“  ë¬¸ì„œ ë¡œë“œ
    try:
        sample_dir = "sample"
        print(f"ğŸ“ ë””ë ‰í„°ë¦¬ì˜ ëª¨ë“  ë¬¸ì„œ ë¡œë”©: {sample_dir}")
        all_documents = loader.load_directory(sample_dir, show_progress=True, recursive=True)
        
        if all_documents:
            dir_info = loader.get_document_info(all_documents)
            print(f"ğŸ“ˆ ë””ë ‰í„°ë¦¬ í†µê³„:")
            for key, value in dir_info.items():
                print(f"   {key}: {value}")
        else:
            print("   ë¬¸ì„œë¥¼ ì°¾ê±°ë‚˜ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            
    except Exception as e:
        print(f"âŒ ë””ë ‰í„°ë¦¬ ë¡œë”© ì˜¤ë¥˜: {e}")
    
    # ì˜ˆì œ 3: í¸ì˜ í•¨ìˆ˜ ì‚¬ìš©
    print("\nğŸš€ í¸ì˜ í•¨ìˆ˜ ì‚¬ìš©:")
    try:
        quick_docs = load_documents("sample/êµ­ê°€ë³„ ê³µê³µë¶€ë¬¸ AI ë„ì… ë° í™œìš© ì „ëµ.pdf", 
                                  chunk_size=500, 
                                  split_documents=True)
        print(f"âœ… ë¹ ë¥¸ ë¡œë“œ: {len(quick_docs)}ê°œ ì²­í¬")
    except Exception as e:
        print(f"âŒ ë¹ ë¥¸ ë¡œë“œ ì˜¤ë¥˜: {e}")